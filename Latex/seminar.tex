% Master-Konferenzseminar, FH Suedwestfalen
%
% Vorlage ist xelatex-kompatibel
% Literatur in seminar.bib
% Build-Prozess:
%  xelatex seminar; bibtex seminar; xelatex seminar; xelatex seminar
%
%
\documentclass[twoside,a4paper]{IEEEtran}
\usepackage{fontspec}
\usepackage[ngerman]{babel}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{multicol}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{numberstyle=\tiny\color{blue},keywordstyle=\color{blue}}
\usepackage{blindtext}  % <-- kann weg!
% Custom
\renewcommand{\thepage}{X-\arabic{page}}
\begin{document}
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Erkennung japanischer Schriftzeichen \\mittels Maschinellen Lernens%
    \thanks{%
    Dieser Beitrag entstand im 
    Rahmen des Master"=Konferenzseminars \emph{info@swf 2022}, das im
    Wintersemester 2021/22 vom Fachbereich Informatik und
    Naturwissenschaften der Fachhochschule Südwestfalen
    durchgeführt wurde.}}
% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
  \IEEEauthorblockN{Sebastian Schmidt\\ }
  \IEEEauthorblockA{Fachhochschule Südwestfalen}
}

\maketitle


%%% Zusammenfassung
\begin{abstract}
Das japanische Schriftsystem ist mit drei verschiedenen Alphabeten (Hiragana, Katakana, Kanji) eines der komplexesten Schriftsysteme der Welt. Eine Person muss dabei ca. 2000 Schriftzeichen erlernen, um eine Zeitung vollständig verstehen zu können. In dem Paper \cite{RHC} von Charlie Tsai wurde eine Erkennung mittels der VGGNet, speziellen Architekturen Konvolutionaler Neuronaler Netze, erprobt. Im Ansatz wurde dabei sowohl die separate Erkennung aller Alphabete als auch die gemeinsame aller möglichen Schriftzeichen erforscht. Dabei konnten Erkennungsgenauigkeiten von bis zu 99,53\% erreicht werden. In diesem Paper sollen die Ergebnisse nachvollzogen und mit weiteren Mitteln die Qualität der Modelle überprüft werden. Dies soll den Einsatz und Vergleich weiterer Netzarchitekturen wie beispielsweise der Vision Transformer oder ResNet ermöglichen. Zusätzlich wurden weitere moderne VGG- und ResNet Architekturen gesichtet und mit den implementierten Mitteln überprüft. Auch ein Einsatz weiterer Datensätze aus der vorliegenden Datenbank wurde erprobt. Im Paper konnte der State of the Art F1-Score unter einer Verdreifachung der erkannten Schriftzeichen reproduziert werden.
\end{abstract}


%%% Text beginnt
\section{Einleitung}
Die japanische Sprache ist eine der komplexesten Sprachen der Welt. Besonders für Muttersprachler europäischer Sprache sind die hohen Abweichungen in Aussprache, Grammatik und Schriftsystem ein Grund für große Probleme beim Erlernen der Sprache. Laut dem \emph{Foreign Service Institute} (FSI) ist Japanisch, neben Sprachen wie Arabisch, Koreanisch und Chinesisch, eine Kategory IV Sprache, welche mindestens 2200 Lernstunden für englische Muttersprachler benötigen und damit als eine der schwierigsten Sprachen der Welt gilt \cite{FSI}.

Geteilt in drei Alphabete (Hiragana, Katakana, Kanji) muss ein Schüler des Japanischen über 2000 Schriftzeichen erlernen, um eine Zeitung ohne Hilfe lesen zu können. Dazu kommt zusätzlich im Fall der Kanji nicht nur die Bedeutung sondern auch verschiedene Lesungen. Die Anzahl dieser kann von einigen wenigen bis in die Dutzende reichen \cite{ABOUT_JAPANESE}.

Schüler des Japanischen können heutzutage auf viele Tools zurückgreifen, um ihren Lernvorgang zu unterstützen. Auch gibt es einige Apps, welche die automatische Erkennung von japanischen Schriftzeichen ermöglichen. In Charlie Tsais Paper \cite{RHC} konnte dieser bis zu 99,53\% aller Schriftzeichen der \emph{ETL Character Database}, einer Datenbank von handgeschriebenen japanischen Schriftzeichen, korrekt klassifizieren \cite{RHC}. Diese Ergebnisse sollen in diesem Projekt durch den Einsatz der genutzten \emph{VGGNet} nachvollzogen und falls möglich durch die Nutzung weiterer Techniken wie \emph{Vision Transformer} oder \emph{ResNet} verbessert werden. Dazu wird zunächst auf die Grundlagen dieser Arbeit eingegangen, um im Folgenden die eingesetzten Daten darzustellen. Danach können verschiedene Modelle für das maschinelle Lernen aufgestellt und deren Ergebnisse vorgestellt werden. Schließlich wird ein Fazit und ein Ausblick auf mögliche nächste Arbeiten gegeben.

Für den praktischen Teil der Arbeit wurde \emph{Python 3} mit \emph{Keras} und \emph{Tensorflow 2} verwendet.
\section{Grundlagen}
Im folgenden Abschnitt sollen kurz die Grundlagen der eingesetzten Modelle dargestellt werden. Dabei wird zunächst auf die \emph{Konvolutionalen Neuronalen Netze} und dann auf die \emph{Vision Transformer} eingegangen. Grundlagen zu Neuronalen Netzen werden dabei als gegeben angesehen.
\subsection{Konvolutionale Neuronale Netze} %11.11
\emph{Konvolutionale Neuronale Netze} oder \emph{Convolutional Neural Networks} (\emph{CNN}) sind Neurale Netze, die vom visuellen Cortex des Gehirns inspiriert wurden. Hierbei sind Neuronen in unteren Schichten für die Erkennung primitiver Formen verantwortlich, während Neuronen in höheren Schichten auf immer komplexere Muster reagieren \cite[S.360]{MACHINE_LEARNING}. 

\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{cnn}
	\caption{Aufbau einer konvolutionalen Schicht mit Schrittweite eins und Zero Padding \cite[S.362]{MACHINE_LEARNING}}
	\label{bild1}
\end{figure}

In der Praxis haben sich die sogenannten konvolutionalen Schichten mit einem bestimmten Wahrnehmungsfeld mehrere Pixel etabliert, welche  ihre Eingabe mit einer Schrittweite (Stride) analysieren und so aus den Pixeln eine neue Ausgabe erzeugen. Dieser Vorgang lässt sich in der Abbildung \ref{bild1} erkennen. Hier wird eine unterliegende Eingabe mit einem gewissen Wahrnehmungsfeld untersucht und so eine neue Ausgabe erzeugt. Diese Eingaben können beispielsweise Bilder oder Ausgaben vorherigen konvolutionaler Schichten sein. Die Eingabe des Wahrnehmungsfelds wird mittels eines sogenannten Filters gewichtet, bevor ein Ausgabepixel erzeugt wird. Filter lassen sich als kleine Bilder visualisieren, welche auf bestimmte Formen in der Ausgangsschicht reagieren \cite[S.361-363]{MACHINE_LEARNING}. 

Je konvolutionaler Schicht können nun mehrere Filter sogenannte Feature Maps auftreten, welche die Erkennung verschiedener Formen in der vorherigen Schicht erlauben und in tieferen Schichten die Kombination vorheriger Features ermöglichen. Tatsächlich erstreckt sich nun das Wahrnehmungsfeld über alle vorherigen Feature Maps, was in einer dreidimensionalen Gewichtsmatrix resultiert \cite[S.364-365]{MACHINE_LEARNING}. Diese Art des Vorgehen erlaubt zusätzlich eine Analyse von Farbbildern mit mehreren Kanälen. Ein solches Verfahren mit einem Stapeln mehrerer Feature Maps und einer dreidimensionalen Gewichtsmatrix lässt sich in Abbildung \ref{bild2} erkennen. Es wird ersichtlich, dass sich die vorher zweidimensionalen Wahrnehmungsfelder über drei Dimensionen erstrecken und die Ausgabe einer Feature Map erzeugen. Es ist wichtig zu beachten, dass die Feature Maps unabhängig voneinander die Ausgabe der vorherigen Schicht analysieren. 

\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{cnn_example}
	\caption{Aufbau eines \emph{CNN} mit drei Schichten und vielen Feature Maps \cite[S.365]{MACHINE_LEARNING}}
	\label{bild2}
\end{figure}

Typischerweise wird durch eine passende Wahl von Strides und Filtern eine Komprimierung der Bildgröße bei Steigerung der Feature Maps angestrebt. Eine Komprimierung kann dabei aber nicht nur durch Strides, sondern auch durch Pooling Layer geschehen. Ein Pooling Layer betrachtet in der Regel einen gewissen zweidimensionalen Bereich einer Ausgabe und komprimiert diesen auf eine vom Typ des Layers abhängige Weise. Hierdurch entsteht eine Komprimierung der Ausgabe der vorherigen konvolutionalen Schicht. Eine typische Art von Pooling ist das Max-Pooling bei welchem die größte Zahl des betrachteten Bereichs übernommen wird \cite[S.369-370]{MACHINE_LEARNING}. In den meisten Architekturen von \emph{CNN}s wird am Ende die Ausgabe der letzten konvolutionalen Schicht geebnet und mit einem Feed-Forward-Netz, die typische Variante Neuronaler Netze mit Schichten von Neuronen \cite[S.263]{MACHINE_LEARNING}, verbunden, welches dann Aufgaben wie Klassifikationen vornehmen kann \cite[S.371]{MACHINE_LEARNING}. Einige verbreitete Architekturen von \emph{CNN}s, welche auch in diesem Projekt erprobt wurden, sind das \emph{ResNet} und \emph{VGGNet} \cite[S.378-381]{MACHINE_LEARNING}.

\subsection{Vision Transformer} %12.11

\emph{Vision Transformer} (ViT) sind eine neue Technik des Maschinellen Lernens auf Basis der \emph{Transformer} \cite{TRANSFORMERS}. Diese basieren auf dem Hauptprinzip der sogenannten Attention und werden vorwiegend im Natural Language Processing eingesetzt, in welchem sie die vorher prävalenten \emph{Rekurrenten Neuronalen Netze} (\emph{RNN}) weitgehend ablösen konnten \cite{TRANSFORMERS}. Wurden diese in Encoder-Decoder Netzen, spezielle Neuronale Netze bei welchen aus einer Eingabesequenz eine Ausgabesequenz erzeugt wird \cite[S.388-389]{MACHINE_LEARNING}, eingesetzt, so wurde bis zur Nutzung von Attentions nur ein letzter Zustand des Encoders als Eingabe im Decoder genutzt. Dies hat sich nach aktueller Forschung als Bottleneck herausgestellt, welches \emph{Rekurrenten Neuronale Netze} besonders bei längeren Sätzen stark behindert hat \cite[S.2]{TRANSFORMERS}. 

\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{attention_visualized}
	\caption{Beispiel für erlernte Attention zwischen einem englischen und französischen Satz \cite[S.6]{RNN_ATTENTION}}
	\label{bildAttention}
\end{figure}

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=2.1\columnwidth]{vitimg}
	\caption{Grundsätzlicher Aufbau eines Vision Transformers \cite[S.3]{VIT}}
	\label{vitimg}
\end{figure*}
Stattdessen wurden neue \emph{RNN}s entworfen, welche es dem Decoder über eine erlernbare Attention erlauben sich in jedem Schritt auf andere Aspekte aller Zustände des Encoders zu konzentrieren \cite[S.4]{RNN_ATTENTION}. Eine Repräsentation dieses Vorgangs lässt sich in Abbildung \ref{bildAttention} erkennen. In dieser wird über zunehmende Grauwerte die Attention zwischen einem englischen und französischen Satz dargestellt. In weiteren Forschungen im Natural Language Processing hat sich herausgestellt, dass reine Netzwerke basierend auf Attention \emph{RNN}s als State of the Art können \cite[S.2]{TRANSFORMERS}. Dies sind die bereits erwähnten \emph{Transformer}.

\emph{Vision Transformer} sind vom Aufbau den \emph{Transformern} im Natural Language Processing sehr ähnlich, besitzen jedoch im Fall von Klassifikation nur den Encoder der Transformer Modelle. Der grundsätzliche Aufbau lässt sich in der Abbildung \ref{vitimg} erkennen. Dabei werden zunächst die Eingabebilder in mehrere zweidimensionale Bereiche einer vordefinierbaren Größe unterteilt, wobei ähnlich wie bei den \emph{CNN}s ein Stride zum Überlappen von Bildbereichen genutzt werden kann. Danach werden die Eingabedaten über eine trainierbare lineare Projektion auf D, eine konstante vordefinierte Vektorgröße in allen Schichten des Transformers, Dimensionen reduziert und damit die sogenannten Patch-Embeddings generiert. Zusätzlich werden eindimensionale Positionskennungen der Patches hinzugefügt, welche eine Repräsentation der Reihenfolge der Bildteile im Modell ermöglicht. Schließlich wird an den Anfang der Daten ein spezielles Class-Token für das Erlernen der vorhandenen Klasse angefügt \cite[S.3]{VIT}. 

Die Schichten des Encoders bestehen im Kern aus mehreren Multiheaded Self-Attentions, darauffolgende zweischichtigen Multilayer Perceptrons und Residual Connections zwischen den inneren Schichten. Weiterhin wird eine Layernormalisierung vor jeder Multiheaded Self-Attention und jedem Multilayer Perceptron ausgeführt. Am Ende wird aus dem Encoder schließlich der Statusvektor an der Position des Class-Token entnommen und zur Klassifikation in ein weiteres Feed-Forward-Netz mit einer einzelnen Schicht geleitet \cite[S.3-4]{VIT}. 

In den Ergebnissen zeigt sich, dass \emph{Vision Transformer} ähnliche Ergebnisse wie aktuelle \emph{CNN}s erzeugen können, wenn sie auf ausreichend großen Datensätzen trainiert werden. Dabei benötigen sie zusätzlich deutlich weniger Rechenkapazitäten \cite[S.5]{VIT}. Dies wird besonders ersichtlich, wenn ein \emph{Vision Transformer} auf Datensätze verschiedener Größe vortrainiert wird. Hier hat sich gezeigt, dass ein Training auf dem JFT-300M, Googles Hauseigener Datensatz mit über 300 Millionen Bildern, eine grundlegend bessere Genauigkeit als State of the Art \emph{CNN}s erzeugt, während der vergleichsweise kleine ImageNet Datensatz mit 14 Millionen Bildern \cite{IMAGENET} die Genauigkeit schlechter als \emph{CNN}s werden lässt \cite{JFT}.
\section{Übersicht der Daten} %13.11
Mit der Darstellung grundlegender Technologien, kann nun auf die verwendeten Datensätze eingegangen werden. Dies geschieht im folgenden Kapitel.

Die genutzten Bilder von japanischen Schriftzeichen aus \cite{RHC} sind aus der \emph{ETL Character Database} \cite[S.2-3]{RHC}, einer Datenbank erstellt zwischen 1973 und 1984 aus ca. 1,2 Millionen handgeschriebener und gedruckter Hiragana, Katakana, Kanji und weiterer Zeichen. Sie wird geteilt in neun Datensätze ETL-1 bis ETL-9 mit unterschiedlichen Inhalten \cite{ETL}. Jedes Zeichen liegt als 64 x 64 Pixel Grauwertdaten vor, welche zentriert sowie mit einer speziellen Methode in ein Binärbild umgewandelt wurden \cite[S.3]{RHC}. Eine Auswahl dieser Zeichen wird in Abbildung \ref{kana} dargestellt. Aufgrund der besonderen Formate und Spezifikationen der Datensatzdateien, musste ein spezielles Pythonskript hinzugezogen werden, um die Daten in einfacher nutzbare PNG-Dateien umzuwandeln \cite{ETL_FORMATS}.
 
\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{kana}
	\caption{Extrahierte und invertierte Schriftzeichen aus der \emph{ETL Character Database} \cite[S.1]{RHC}}
	\label{kana}
\end{figure}

Aus den umgewandelten Daten wurden zwei verschieden aufgebaute Datensätze erstellt. Der Erste ist an \cite{RHC} angelehnt und beinhaltet lediglich den ETL-1 Datensatz mit Katakana und den ETL-8 Datensatz mit Hiragana sowie Kanji \cite[S.3]{RHC}. Der Aufbau von diesem Datensatz mit Unterteilungen in Hiragana, Katakana, Kanji und sonstiger Zeichen sowie das arithmetische Mittel der Anzahl der Datenpunkte pro Schriftzeichen und deren Standardabweichung lässt sich in der Tabelle \ref{data_tsai} erkennen.
\begin{table}[!htb]
	\caption{Aufbau des ersten Datensatz nach \cite{RHC}}
	\label{data_tsai}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Alphabet & Klassen & Datenpunkte & Mean & Stddev\\
		\hline
		\hline
		Hiragana & 75 & 12.075 & 161 & 0\\
		\hline 
		Katakana & 48 & 71.959 & 1.499 & 342\\
		\hline
		Kanji & 881 & 141.841 & 161 & 0\\
		\hline
		Sonstige & 0 & 0 & 0 & 0\\
		\hline
		\hline
		Gesamt & 1.004 & 225.875 & 224 & 295\\
		\hline
	\end{tabular}
\end{table}

Es wird ersichtlich, dass Katakana im Gegensatz zu Kanji und Hiragana von der Anzahl der Datenpunkte stark überrepräsentiert sind. Eine Nutzung von Klassengewichtungen, um die unterrepräsentierten Daten hervorzuheben, sollte sich daher als sinnvoll erweisen. Weiterhin stellt sich dies auch für die Katakana als möglicherweise sinnvoll heraus, da diese eine gewisse repräsentative Streuung aufweisen. Weil der ETL-1 Datensatz ebenfalls Zeichen mit Beugungen (Da\-ku\-ten und Han\-da\-ku\-ten; Veränderungen an Zeichen, welche sich mit den deutschen Umlauten vergleichen lassen) beinhaltet, besitzt der Datensatz deutlich mehr Klassen für Hiragana als Katakana, obwohl beide Alphabete sehr ähnlich aufgebaut sind. Im Gegensatz zu \cite{RHC} sind hier zwei Probleme zu erkennen. Zum einem hat sich die Anzahl der Datenpunkte seit \cite{RHC} um ca. 1000 erhöht \cite[S.3]{RHC}. Da hier kein Fehler beim Vorverarbeiten der Daten erkannt werden konnte, könnte es sich um ein Hinzufügen von Daten seitens der Verwalter der \emph{ETL Character Database} handeln, was auch in einer Steigerung der Klassen der Kanji um drei resultiert. Weiterhin finden sich drei Katakanatypen weniger wieder. Dies hängt mit der Darstellung ausgestorbener Zeichen im ETL-1 Datensatz zusammen, welche die Buchstaben \textit{yi} zu \textit{i}, \textit{ye} zu \textit{e} und \textit{wu} zu \textit{u} werden lassen \cite{ETL}. Hier wurden im Gegensatz zu \cite{RHC} die daraus resultierenden Beispiele zu den passenden Vokalen umgeschrieben \cite[S.3]{RHC}. Eine gleichzeitge Erhöhung der Kanji und Verringerung der Katakana um drei hat sich nach Prüfungen als zufällig herausgestellt. 

Mit diesem ersten Datensatz können einige praktische Probleme nicht gelöst werden. Soll Maschinelles Lernen zum Übersetzen von Sprache eingesetzt werden, so ist es nötig, die meisten Aspekte dieser Sprache zu berücksichtigen. Heutzutage hat sich das lateinische Alphabet in viele Aspekte der japanischen Sprache etabliert. Eine reine Erkennung japanischer Zeichen ist daher für praktische Anwendungen eher weniger sinnvoll. Auch Satzzeichen und Sonderzeichen wurden im Datensatz nicht berücksichtigt, obwohl diese für ein späteres Verständnis ganzer Sätze essenziell sind. Zuletzt beinhaltet der Datensatz lediglich 881 der 2136 üblich genutzten Kanji \cite[S.3]{RHC}. Da die \emph{ETL Character Database} noch weitere Datenpunkte beinhaltet, wurde sich entschieden in einem weiteren Datensatz die gesamte Datenbank zu nutzen. Der Aufbau dieser Gesamtauswahl lässt sich in Tabelle \ref{data_total} erkennen.

\begin{table}[!htb]
	\caption{Aufbau des zweiten Datensatz als Gesamtauswahl der \emph{ETL Character Database}}
	\label{data_total}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Alphabet & Klassen & Datenpunkte & Mean & Stddev\\
		\hline
		\hline
		Hiragana & 77 & 67.335 & 874 & 437\\
		\hline 
		Katakana & 73 & 147.237 & 2.017 & 1.496\\
		\hline
		Kanji & 2.967 & 782.561 & 264 & 83\\
		\hline
		Sonstige & 74 & 175.308 & 2.369 & 1.013\\
		\hline
		\hline
		Gesamt & 3.191 & 1.172.441 & 367 & 507\\
		\hline
	\end{tabular}
\end{table}
Wie ersichtlich wird, beinhaltet die gesamte \emph{ETL Character Database} noch deutlich mehr Daten, welche für das Maschinelle Lernen genutzt werden können. So konnten die vorliegenden Kanji verdreifacht und die durchschnittlichen Datenpunkte pro Kanji nahezu verdoppelt werden. Weiterhin sind die Hiragana nicht mehr stark unterrepräsentiert und sonstige Zeichen wie aus dem lateinischen Alphabet oder Satzzeichen können erlernt werden. Eine größere Schwankung in der Durchschnittszahl der Datenpunkte sowie eine höhere Streuung dieser macht auch hier wieder eine Nutzung von Klassengewichtungen eine sinnvolle Ergänzung.

Eine beliebte Möglichkeit das Overfitting von Modellen zu verringern sowie die Toleranz gegenüber Daten in der Praxis zu verbessern ist die sogenannte Data Augmentation. Dabei können durch bestimmte Transformationen aus vorliegenden Daten neue generiert werden. Typische Vorgehensweisen bei Bildern sind das Skalieren, Verschieben oder Rotieren der Daten. Auch eine Spiegelung kann sich bei manchen Bilddaten als sinnvoll erweisen, jedoch würde dies die Bedeutung von Schriftzeichen zerstören. Werden Transformationen miteinander kombiniert, so kann die Anzahl der vorhandenen Daten um ein Vielfaches erhöht werden \cite[S.311]{MACHINE_LEARNING}. Im Code des Projekts können Skalierungen und Rotationen verwendet werden, um die Daten künstlich zu erhöhen. Dies geschieht über Transformationsmatrizen, welche mit der Bibliothek \emph{CV2} generiert und angewendet wurden. Eine Generation von Daten mit 25 verschiedenen Transformationsmatrizen wurde erprobt. Diese würden bei der beispielhaften Anwendung auf dem Kanji mit der Bedeutung \textit{zwei Stück} die Ausgabe in der Abbildung \ref{rot} erzeugen. Dort wird die Skalierung und Rotation ersichtlich, welche in einer Vergrößerung der Daten um den Faktor 25 resultieren würde.

\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{rot_scale}
	\caption{Transformationen des Kanji \textit{zwei Stück}}
	\label{rot}
\end{figure}

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=2.1\columnwidth]{conv_net_architectures}
	\caption{Genutzte \emph{VGGNet} Architekturen im Paper von Charlie Tsai \cite[S.3]{RHC}}
	\label{vggnet}
\end{figure*}

\section{Eingesetzte Modelle}
In den vorherigen beiden Kapitel wurde die technologische- sowie Datengrundlage für die Arbeit gelegt. Diese Grundlagen erlauben nun eine eigene Definition von Modellen, welche im nächsten Kapitel hervorgehoben werden. 

Im Gegensatz zum Paper \cite{RHC}, wurde aufgrund der guten Ergebnisse bei einer gemeinsamen Erkennung aller Alphabete die Nutzung von separaten Modellen für Kanji, Katakana und Hiragana ausgeschlossen. Dieses Vorgehen würde ein zusätzliches Modell zur Unterscheidung von Alphabeten benötigen und daher insgesamt vier Modelle voraussetzen. Da die Ergebnisse für die Erkennung aller Alphabete laut \cite{RHC} bereits bei 99,53\% der Testdaten lagen, wurde sich gegen einen separaten Ansatz entschieden, um den Fokus auf die Erkennung aller Zeichen zu legen \cite[S.4-6]{RHC}. Aufgrund der teilweise hohen Streuung der Häufigkeiten von Datenpunkten pro Schriftzeichen, wurden alle Ansätze mit Klassengewichtungen erprobt. Dies ist nötig, damit jedes Schriftzeichen unabhängig von der Häufigkeit im Datensatz in etwa gleich gut erkannt werden kann.

Während des Lernvorgangs wurden die Daten in 80\% Trainings- und jeweils 10\% Validierungs- und Testdaten aufgeteilt. Alle Modelle wurden mit einem \emph{Adam-Optimizer}, einem beliebten Optimierungsalgorithmus für Neuronale Netze, trainiert, welcher auch in \cite{RHC} genutzt wurde \cite[S.4]{RHC}. Die Startlernrate wurde dabei von Modell zu Modell angepasst. Es wurde weiterhin eine zerfallende Lernrate genutzt, welche alle 20 Epochen die Rate auf ein 0,9-Faches des ursprünglichen Wertes reduziert. Dies ist am Vorgehen von \cite{RHC} angelehnt \cite[S.4]{RHC}. Die Genauigkeit und der Verlust auf den Validierungsdaten wird mit jeder Epoche berechnet und dient als Maß des Fortschritt während des Trainings. Ein Training wird automatisch abgebrochen, falls nach 20 Epochen keine Verbesserung des Verlusts auftritt. Nach dem Training wird eine Übersicht generiert, welche den ungewichteten Durchschnitt des F1-Scores aller Klassen auf den Validierungsdaten als auch für einen späteren Vergleich zwischen den Modellen auf den Testdaten beinhaltet. Zusätzlich wird die Gesamtgenauigkeit ausgegeben.

\subsection{VGGNet}  %14.11 --- 

Die Architektur der \emph{VGGNet} wurde in Simonyans und Zissermans Paper \cite{simonyan2015deep} entworfen und konnte 2015 State of the Art Genauigkeiten mit einem relativ einfachen sowie flexiblen Aufbau \emph{Konvolutionaler Neuronaler Netze} erreichen \cite[S.1]{simonyan2015deep}. Sie arbeiten in den konvolutionalen Schichten typischerweise mit einem Wahrnehmungsfeld von 3 x 3 Elementen. Weiterhin nutzen sie ein 2 x 2 Max-Pooling mit einem Stride von 2. Das Max-Pooling tritt dabei nach einigen aber nicht jeder Schicht auf. Danach folgen zwei Feed-Forward Schichten mit 4096 Neuronen und schließlich eine Feed-Forward Schicht mit Neuronen in Höhe der Anzahl der vorliegenden Klassen \cite[S.2]{simonyan2015deep}.

In \cite{RHC} wurden 11 \emph{VGGNet} artige Netze umgesetzt, welche in der Abbildung \ref{vggnet} von links nach rechts nach Netzgröße geordnet dargestellt werden. Diese weisen im Vergleich zum originalen \emph{VGGNet} Paper einige Veränderungen in der Struktur auf. Zum einen liegen die Daten nur mit einem 64 x 64 Pixel Grauwertkanal vor \cite[S.3]{RHC}. Dadurch wird die Größe der Eingabedaten im Gegensatz zum originalen \emph{VGGNet} Paper mit 224 x 224 Pixel RGB-Bildern sehr viel kleiner \cite[S.2]{simonyan2015deep}. Weiterhin wurde die Anzahl der Neuronen und Schichten im Feed-Forward Teil von Modell zu Modell angepasst \cite[S.3]{RHC}. Aufgrund der guten Ergebnisse der Klassifikation in \cite{RHC}, wurde ein besonderer Fokus auf die Modelle \emph{M7-1}, \emph{M7-2}, \emph{M9} und \emph{M12} gesetzt \cite[S.5]{RHC}. Alle diese Netzen wurden nach \cite{RHC} mit einem Dropout, eine Möglichkeit eine gewisse Zahl von Neuronen während des Trainings auszuschalten, um Overfitting also ein Auswendiglernen der Eingabedaten zu verhindern, versehen \cite[S.306-307]{MACHINE_LEARNING}. Dieser wurde allerdings aufgrund schlechterer Ergebnisse bei höheren Raten auf 25\% verringert und nur nach jeder Gruppe von Schichten platziert \cite[S.2]{RHC}. Mit Batch-Normalisierung wurde zusätzlich eine weitere Technik zum Verhindern von Overfitting eingesetzt \cite[S.282-283]{MACHINE_LEARNING}. Dank der direkten Unterstützung der \emph{Keras Applications API} und einer Möglichkeit des Transfer Learnings bereits erlernter Modellgewichte, wurden zusätzlich die von Keras implementierten \emph{VGG16} und \emph{VGG19} Netze erprobt \cite{vgg_keras}.

\begin{figure}[!htb]
	% \centering
	\includegraphics[width=\columnwidth]{residual}
	\caption{Beispiel einer Abkürzungsverbindung über zwei Schichten \cite[S.2]{resnet}}
	\label{res}
\end{figure}

\subsection{ResNet} %15.11
Die \emph{ResNet} sind spezielle Architekturen Neuronaler Netze, welche mittels Restdarstellungen (Residual Representations) durch Abkürzungsverbindungen (Shortcut Connections) die Performanz tiefer Neuronaler Netze und im besonderen \emph{Konvolutionaler Netze} verbessern sollen \cite[S.1-2]{resnet}. Dabei wird eine Ausgabe, welche an einem bestimmten Punkt im Netzwerk ausgegeben wurde, an einem späteren Punkt identisch vor der Aktivierungsfunktion in die Ausgabe der tieferen Schicht eingerechnet \cite[S.4]{resnet}. Diese Funktionsweise lässt sich in der Abbildung \ref{res} am Beispiel zweier Schichten erkennen.

Ein solches Vorgehen soll den bei tiefen Neuronalen Netzen beobachteten Effekt des Zerfalls von Genauigkeiten bei einer zunehmenden Steigerung der Schichtenzahl mildern \cite[S.1]{resnet}. \emph{Keras} bietet in der \emph{Applications API} für diese Netzarchitekturen sechs vordefinierte Modelle, welche 50, 101 bzw. 152 Schichten aufweisen \cite{resnet_keras}. Im Besonderen wurde beim Trainieren ein Fokus auf die V2 Varianten dieser Modelle gesetzt, bei welchen es sich um eine Verbesserung des originalen Ansatz der \emph{ResNet} handelt \cite{resnetv2}.
\subsection{Vision Transformer} %15.11
\emph{Vision Transformer} wurden in vier verschiedenen Varianten erprobt, welche im zugehörigen Paper dargestellt wurden. Es wurden dabei Modelle mit 12 (Base), 24 (Large) und 32 (Huge) Schichten vorgestellt, wobei die größte Variante aufgrund der nötigen Datenmenge nicht erprobt wurde. Alle Varianten der \emph{ViT} mit ihren Parametern lassen sich in der Tabelle \ref{vit_sizes} erkennen. \cite[S.5]{VIT}. Die Base- und Large-Varianten wurden in der Version 16 und 32 erprobt, bei welcher es sich um die Dimensionen der Input Patches handelt. Bei der Version 16 sind diese 16x16 Pixel, während die Größen bei der Version 32 bei 32x32 Pixeln liegen \cite[S.5]{VIT}. 

\begin{table}[!htb]
	\caption{Vordefinierte Varianten der \emph{Vision Transformer} Modelle \cite[S.5]{VIT}}
	\label{vit_sizes}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Schichten & Größe D & MLP Größe & Parameter\\
		\hline
		\hline
		Base & 12 & 768 & 3072 & 86M\\
		\hline
		Large & 24 & 1024 & 4096 & 307M\\
		\hline
		Huge & 32 & 1280 & 5120 & 632M\\
		\hline 
	\end{tabular}
\end{table}

In der Praxis wurden diese über das Pythonpaket \emph{vit-keras} umgesetzt, welches eine einfach und ähnlich zur \emph{Keras Applications API} nutzbare Definition der \emph{ViT}-Modelle ermöglicht. In diesem wurden alle nötigen Varianten implementiert und die Möglichkeit offen gehalten vorerlernte ImageNet-Gewichtungen zu laden \cite{git_vit}. Dies ist abschließend aufgrund der Unterschiede zwischen den ImageNet-Daten und verwendeten Kanji-Daten in der finalen Arbeit nicht geschehen. Zusätzlich musste für die vorliegende Implementierung der \emph{Vision Transformer} die Grauwertdaten der Kanji in RGB Daten mit drei Kanälen umgewandelt werden. 
\section{Ergebnisse} %16.11
Die Ergebnisse aller trainierten Modelle werden im Folgenden dargestellt. Hierbei sei zu beachten, dass aufgrund Zeitmangels einige Modelle nicht bis zum automatischen Beenden des Trainings trainiert werden konnten. Diese wurden stattdessen nur 40 Epochen verbessert und sind über ein Asterisk am Modellnamen markiert. Beim Vergleich von Modellen wird der ungewichtete durchschnittliche F1-Score unserer Modelle mit der Genauigkeit des Paper \cite{RHC} verglichen. Dies ergibt sich aufgrund unserer Nutzung von Klassengewichtungen, welche die Genauigkeit in der Tendenz schlechter werden lässt und den durchschnittlichen F1-Score zu einem besseren Vergleichspunkt macht. 

\subsection{VGGNet}

Mittels des Tsai-Datensatz und der definierten \emph{VGG} Netzen konnten sehr ähnliche Ergebnisse zum Paper \cite{RHC} erarbeitet werden. Es wurden für diesen Datensatz insgesamt sechs \emph{VGG} Modelle trainiert, deren Ergebnisse in der Tabelle \ref{vgg_ergebnis_tsai} dargestellt werden.

\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der \emph{VGGNet} Modelle für den Tsai-Datensatz in Prozent}
	\label{vgg_ergebnis_tsai}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline
		\hline
		M7-1 & 98,35 & 98,37 & 98,92 & 98,87\\
		\hline
		M7-2 & 98,04 & 98,25 & 98,95 & 98,95\\
		\hline
		M9 & 98,82 & 99,00 & 99,23 & 99,25\\
		\hline 
		M12 & \textbf{99,00} & \textbf{99,07} & \textbf{99,34} & \textbf{99,31}\\
		\hline 
		VGG16 & 97,97 & 98,21 & 98,64 & 98,63\\
		\hline 
		VGG19 & 97,68 & 97,58 & 98,32 & 98,10\\
		\hline 
	\end{tabular}
\end{table}

Das beste Modell \emph{M7-1} im Paper \cite{RHC} konnte statt mit einer Genauigkeit von 99,39\% und 99,53\% nur einen durchschnittlichen F1-Score von 98,92\% und 98,87\% erreichen. Stattdessen konnte das Modell \emph{M12} mit 99,34\% und 99,31\% sehr vergleichbare Ergebnisse erlangen, während es in \cite{RHC} schlechter abschnitt. Eine Abweichung von über einem halben Prozent im \emph{M7-1} ist bei Modellen, die schnell eine Genauigkeit von über 95\% erreichen enorm und eine zufällige Schwankung ist eher auszuschließen. Es könnten Unterschiede beim Vorverarbeiten der Daten oder bei der Regularisierung, Techniken wie Dropout oder Batch-Normalisierung die einem Overfitting entgegenwirken können \cite[S.27]{MACHINE_LEARNING}, aufgetreten sein. Dies wäre eine mögliche Hypothese für die unterschiedlichen Ergebnisse. Unterschiede bei der Regularisierung könnten dabei ebenfalls eine Erklärung für das besseres Abschneiden des \emph{M12} Modells sein. Die Modelle \emph{M7-2} und \emph{M9} liegen von ihren Ergebnissen zwischen \emph{M7-1} sowie \emph{M12} und weichen nicht viel von den des Papers \cite{RHC} ab. Die zusätzlichen über \emph{Keras Applications API} eingesetzten Modelle \emph{VGG16} und \emph{VGG19} schnitten durchweg schlechter als die vorher definierten Modelle ab. Da diese keine eingebaute Regularisierung bieten \cite{keras_vgg} und sie deutlich größer als die anderen Modelle ausfallen, könnte ein früheres Overfitting eine Erklärung für diese Ergebnisse sein.

\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der VGGNet Modelle für den kompletten Datensatz in Prozent}
	\label{vgg_ergebnis_full}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline 
		\hline 
		M7-1 & 98,81 & 98,82 & 99,41 & 99,43\\
		\hline
		M7-2* & 96,60 & 96,57 & 98,36 & 98,38\\
		\hline
		M9 & 98,66 & 98,66 & 99,38 & 99,39\\
		\hline 
		M12 & \textbf{99,04} & \textbf{99,07} & \textbf{99,51} & \textbf{99,54}\\
		\hline 
		VGG16* & 97,84 & 97,92 & 98,47 & 98,57\\
		\hline 
		VGG19* & 98,00 & 98,08 & 98,61 & 98,70\\
		\hline 
	\end{tabular}
\end{table}

Für den größeren kompletten Datensatz wurden dieselben \emph{VGG} Netze genutzt, wobei einige Modelle aufgrund hoher Rechenzeit nicht mehr zu Ende trainiert werden konnten. Die Ergebnisse dazu werden in der Tabelle \ref{vgg_ergebnis_full} dargestellt.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\columnwidth]{loss}
	\caption{Verlust des \emph{M7-2} Modells während des Trainings über den kompletten Datensatz.}
	\label{loss}
\end{figure}

Dieser Datensatz konnte trotz seiner Größe nahezu durchweg bessere Ergebnisse wie der Tsai-Datensatz liefern. Hier ist erneut das \emph{M12} Modell das Beste, welches mit 99,51\% und 99,54\% etwas bessere Ergebnisse im Vergleich zu \cite{RHC} liefert. Aber auch das \emph{M7-1} konnte die Ergebnisse verbessern und angleichen. Die anderen Modelle behalten das im vorherigen Abschnitt bezeichnete Verhalten bei, wobei die Ergebnisse leicht verbessert wurden. Es sei zusätzlich zu beachten, dass über eine Weiterführung des Trainings des \emph{M7-2} Modells noch eine leichte Verbesserung stattfinden könnte. Dies ist in Abbildung \ref{loss} zu erkennen, in welcher der Verlust über weiteren Epochen noch weiter sinken könnte.

\subsection{ResNet}

Die Ergebnisse der vorher genannten \emph{ResNet} Modelle \emph{50V2}, \emph{101V2} und \emph{151V2} aus der \emph{Keras Applications API} für den Tsai-Datensatz sind in der Tabelle \ref{resnet_ergebnis_tsai} dargestellt.

\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der \emph{ResNet} Modelle für den Tsai-Datensatz in Prozent}
	\label{resnet_ergebnis_tsai}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline
		\hline
	 	50V2 & 98,42 & 98,47 & 98,83 & 98,78\\
		\hline
		101V2 & 98,55 & 98,57 & 98,87 & 98,80\\
		\hline
		151V2 & \textbf{98,62} & \textbf{98,74} & \textbf{98,95} & \textbf{98,96}\\
		\hline 
	\end{tabular}
\end{table} 

Das \emph{151V2} konnte von diesen die besten Ergebnisse erreichen. Diese sind mit 98,95\% und 98,96\% in etwa vergleichbar mit den Ergebnissen des \emph{M7-1}, jedoch damit geringer als die des \emph{M-12} Modells. Die anderen beiden Modelle erreichen leicht schlechtere Ergebnisse als das \emph{151V2}. 

Für die Resultate des kompletten Datensatz, welche in der Tabelle \ref{resnet_ergebnis_full} dargestellt werden, sieht dieses Bild etwas anders aus. Hier liefert das \emph{101V2} Modell leicht bessere Ergebnisse wie das \emph{151V2}, obwohl beide Modelle dieselbe (verkürzte) Zeit trainiert wurden. Es ist dabei allerdings durchaus möglich, dass bei einer Beendigung des Trainings eine deutliche Verbesserung des \emph{151V2} stattfinden würde, was die Resultate zum Tsai-Datensatz spiegelt.
\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der \emph{ResNet} Modelle für den kompletten Datensatz in Prozent}
	\label{resnet_ergebnis_full}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline
		\hline 
		50V2 & \textbf{98,66} & 98,65 & 99,09 & 99,07\\
		\hline
		101V2* & 98,63 & 98,65 & \textbf{99,05} & \textbf{99,11}\\
		\hline
		151V2* & 98,61 & \textbf{98,68} & 99,04 & 99,09\\
		\hline 
	\end{tabular}
\end{table}

Im Gegensatz zu den \emph{VGG}-Netzen findet in der \emph{Keras Applications API} in den \emph{ResNet} Modellen Regularisierung per Batch-Normalisierung statt \cite{resnet_keras}. Zusätzlich nimmt die Genauigkeit beim Tsai-Datensatz mit komplexeren \emph{ResNet} Modellen zu, was auch bei einer Vervollständigung des Trainings beim kompletten Datensatz der Fall seien könnte. Dies sind mögliche Indizien dafür, dass eine Verwendung noch größerer \emph{ResNet} Modelle einen positiven Einfluss auf das Ergebnis haben könnte. Ein Blick auf die geringere Zahl der Modellparameter von \emph{ResNet} Modellen mit 60 Millionen im \emph{152V2} gegenüber von 143 Millionen im \emph{VGG19} unterstützt dieses Indiz noch \cite{keras_app}.
\subsection{Vision Transformer}
Für die \emph{Vision Transformer} wurden vier verschiedene Modelle trainiert und deren Ergebnisse für den Tsai-Datensatz sind in Tabelle \ref{vit_ergebnis_tsai} dargestellt. Diese konnten in keiner Weise die erwarteten Ergebnisse erlangen. Der beste \emph{Vision Transformer} \emph{b32} konnte dabei 83,20\% und 83,58\% durchschnittlichen F1-Score erreichen und ist damit deutlich schlechter als jedes \emph{VGG} oder \emph{ResNet} Modell.
\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der \emph{ViT} Modelle für den Tsai-Datensatz in Prozent}
	\label{vit_ergebnis_tsai}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline
		\hline
		b16* & 65,78 & 65,67 & 65,67 & 71,59\\
		\hline
		b32* & \textbf{74,76} & \textbf{75,63} & \textbf{83,20} & \textbf{83,68}\\
		\hline
		l16* & 69,07 & 69,10 & 74,52 & 74,83\\
		\hline
		l32* & 70,75 & 71,28 & 81,87 & 82,13\\
		\hline 
	\end{tabular}
\end{table}

Ähnlich sind die Ergebnisse für den kompletten Datensatz, bei welchen der \emph{b32} \emph{Vision Transformer} 88,61\% und 88,79\% erreichen konnte. Hier ist allerdings bereits zu erkennen, dass eine Steigerung der Daten mittels des kompletten Datensatzes einen positiven Effekt erzeugt. Auf das Training der \emph{l32} und \emph{l16} Transformer mit dem zweiten Datensatz musste aufgrund zeitlicher Restriktionen verzichtet werden.
\begin{table}[!htb]
	\caption{Genauigkeit und F1-Scores der \emph{ViT} Modelle für den kompletten Datensatz in Prozent}
	\label{vit_ergebnis_full}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Modell & Val-Acc & Test-Acc & Val-F1 & Test-F1\\
		\hline
		\hline 
		b16* & 77,12 & 77,11 & 83,55 & 83,42\\
		\hline
		b32* & \textbf{84,50} & \textbf{84,70} & \textbf{88,61} & \textbf{88,79}\\
		\hline 
	\end{tabular}
\end{table}

Für das schlechte Abschneiden der \emph{Vision Transformer} in dieser Arbeit gibt es einige Hypothesen. Zum einen benötigen \emph{Vision Transformer} eine große Anzahl an Bildern, um mit \emph{CNN}s vergleichbare Ergebnisse zu erreichen. Selbst der ImageNet Datensatz mit 14 Millionen Bildern \cite{IMAGENET} erreicht noch schlechtere Ergebnisse mit \emph{Vision Transformern} als mit vergleichbaren \emph{CNN}s \cite{JFT}. Die vorliegenden Datensätze für Kanji besitzen dagegen nur 225.000 und 1,2 Millionen Bilder. Eine weitere Möglichkeit für Probleme beim Training, könnte eine fehlende Eignung der genutzten \emph{ViT} sein. Diese haben unabhängig der vorliegenden Grauwertdaten eine Eingabe von RGB-Daten verlangt, wodurch die Komplexität der Daten durch die Simulation von drei Eingabekanälen gestiegen seien könnte. Eine mögliche Nutzung anderer Bibliotheken für \emph{Vision Transformer}, welche auf Grauwertdaten ausgelegt sind, könnten dieses Problem lösen. Die Nutzung von \emph{Huggingface}, eine beliebte Community und Sammlung für Transformer \cite{huggingface}, und deren Python-API wäre ein möglicher nächster Schritt zur Verbesserung der Vorhandenen und dem Einsatz weiterer Transformer. Hier sollten auch Nachforschungen für spezielle \emph{Vision Transformer} zur Texterkennung getroffen werden. 

\section{Fazit} %17.11
Alles in allem konnte das Projekt erfolgreich umgesetzt werden und die Ergebnisse der Arbeit \cite{RHC} wurden nachvollzogen. Durch eine Simulation des Datensatz von Tsai konnten sehr ähnliche Resultate erreicht werden, wodurch im Folgenden ein Einsatz weiterer Modelle und eines Datensatz möglich war. Eine Nutzung neuer \emph{VGG}, \emph{ResNet} und \emph{Vision Transformer} Modelle mit dem Datensatz, konnten die Ergebnisse nicht verbessern. Besonders die \emph{Vision Transformer} haben sich dabei als problematisch erwiesen. Trotzdem konnte durch die Erweiterung der genutzten Daten auf die gesamte \emph{ETL Character Database}, die Klassifikation auf 3191 Schriftzeichen erweitert werden, während das beste \emph{VGG} Modell die Ergebnisse des Paper \cite{RHC} sogar minimal übertreffen konnte. Eine solche Steigerung und Ausweitung der Ergebnisse könnte ein Einsetzen eines trainierten Modells in einer realen Umgebung zur Erkennung oder Übersetzung von japanischen Schriftzeichen deutlich realistischer machen, was die Relevanz des bearbeiteten Themas hervorhebt.
\section{Ausblick}
In zukünftigen Arbeiten ergeben sich zwei Wege des Vorgehens. Zum einen könnten vorhandene Modelle weiter verbessert und der Einsatz weiterer Techniken erprobt werden. Hierbei ist es denkbar, die \emph{VGG16} und \emph{VGG19} mit Regularisierung oder größere \emph{ResNet} zu erproben. Auch ein Einsatz von \emph{Vision Transformern} mit \emph{Huggingface} und eine Sichtung weiterer Modelle in der \emph{Huggingface} Community, wären ein möglicher nächster Schritt. Der Einsatz der implementierten Techniken der Data Augmentation und die Nutzung der durch diese vergrößerten Datensätze wären ebenfalls ein wichtiges nächstes Vorgehen. Auf ein Training dieser musste aufgrund begrenzter Zeit verzichtet werden, da diese durch die enorme Steigerung der Daten bereits mehr als einen Tag Rechenzeit pro Trainingsepoche in Anspruch genommen haben. Zusätzlich wäre eine Sichtung weiterer Datensätzen für Kanji zur Erweiterung der Datenbasis ein sinnvolles Vorgehen.

Ein weiterer Weg für zukünftige Arbeiten wäre die Nutzung der erstellten Modelle für praktische Anwendungen. Da diese bereits sehr gute Ergebnisse liefern, wäre ein erster Test in einer praktischen Anwendung ein möglicher Schritt. Hierbei könnten die Modelle verwendet werden, um den Nutzern unbekannte Schriftzeichen zu analysieren und in einem Lexikon anzeigen zu lassen. Auch eine Live-Übersetzung japanischer Texte, ähnlich zur App von Google Übersetzer, wäre ein mögliches größeres Projekt bei dem die trainierten Modelle zum Einlesen japanischer Zeichen verwendet werden könnten. In diesen Bereichen wäre auch eine Nutzung von Kontextinformationen möglich, die es aufgrund der Position und Nachbarn eines Zeichens in einem Satz wahrscheinlicher machen, dass ein bestimmter Zeichentyp vorliegt. So ist es beispielsweise relativ wahrscheinlich, dass nach einem Kanjiverb Hiragana als Verbendungen folgen.
%%% Literaturverzeichnis
\bibliographystyle{IEEEtran}
\bibliography{seminar}

%% oder manuell
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

\end{document}